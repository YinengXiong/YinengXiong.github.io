---
layout:     post
title:      "CPD for Salient Object Detection"
subtitle:   "只是复现一下paper"
date:       2019-08-15
author:     "Yineng Xiong"
header-img: "img/2019-08-15/salient_1.jpg"
tags:
    - Deep Learning
    - Salient Object Detection
---

> 很久没有写 Blog 了, 今天写一波

## Salient Object Detection

显著性检测的定义是检测出一张图片中最显著的物体, 其实就可以当作二分类分割问题. 看了几篇paper之后发现用到的网络其实也和语义分割的很相似, 都是`Encoder-Decoder` 结构, 也有类似 `Attention`、 `ASPP` 这种 module, 于是拿来一篇 `CVPR 2019` 的 Paper, 叫 [Cascaded Partial Decoder](https://arxiv.org/abs/1904.08739). 文章宣称的是快速 SOD, `VGG Backbone` 的速度可以达到 __105 FPS__, 算是一个比较快的显著性网络。

## Why So Fast？
显著性检测其实和语义分割问题一样, 都依赖于网络提供精确的物体边界. 文章做了一个实验, 拿一张 Image 过一个VGG, 输出 `Activation Map`, 发现网络的中间层依然可以保留比较准确的边缘信息, 那这样这些浅层的、分辨率高、抽象程度低的 feature 就不需要在 `Decoder` 端使用, 这样就可以节省计算量。

然后在网络的设计上, 作者通过 `Backbone` 上拉出两个分支, 分别接上 `Partial Decoder`, 第一个 `Partial Decoder` 得到初始的 `Saliency Map`, 通过 `HAM` 生成更精细的第二个 `Saliency Map`, 其实第一个可以看成生成的 `Attention`. 

为了加速, 每个`Decoder`分支都先用 `1X1卷积` 将通道数降到 `32`, 然后文章里的 `RFB` 多尺度, 其实就是类似 `Inception` 的东西, 为了节省参数和降低计算量用到了把 `5X5卷积` 替换为 `1X5` 和 `5X1` 的操作, 最后再带上空洞卷积提升感受野.

训练的时候是同时监督两个分支的 `Saliency Map`，注意力分支为检测分支提供精确的 `Attention Map`, 检测分支反过来有助于注意力分支集中于显著对象.

## Training
文章训练用的是 `DUTS-TR`, 也是最常用的显著性数据集，测试用的也是最常见的 `ECSSD + HKU-IS + DUT-OMRON + DUTS-TE + PASCAL-S` 的组合. 先复现一下原paper的精度, __Loss__ 就只放 __MAE__ 的

| Model<br>BackBone| Train<br>Data | DUT<br>OMRON|DUTS-TE|ECSSD|HKU-IS|PASCAL-S|SOD|
| :--------: |  :------: |  :-----: | :------: | :------: |:------: |:------: |:------: |
|CPD(paper)<br>ResNet| DUTS-TE| 0.056 | 0.043| 0.037 | 0.034| 0.072| - |
|CPD(复现)<br>ResNet| DUTS-TE| 0.0549 | 0.0433| 0.0429| 0.0336| 0.0756| 0.1092|

好像还行.

但是又不发paper, 为啥不多拿几个数据集放在一起训一波？

收集了一波显著性开源数据集, __DUTS-TE + MSRA-B + HKU + NJU2K + MSRA10K__,操作起来

| Model<br>BackBone| Train<br>Data | DUT<br>OMRON|DUTS-TE|ECSSD|HKU-IS|PASCAL-S|SOD|
| :--------: |  :------: |  :-----: | :------: | :------: |:------: |:------: |:------: |
|CPD<br>ResNet| DUTS+MSRA-B+<br>HKU+NJU2K+MSRA10K| 0.0508 | 0.0411| 0.037 | 0.0265| 0.0675| 0.1032 |

有一些提升

那再快一点？把backbone改成 __MobileNetV2__

| Model<br>BackBone| Train<br>Data | DUT<br>OMRON|DUTS-TE|ECSSD|HKU-IS|PASCAL-S|SOD|
| :--------: |  :------: |  :-----: | :------: | :------: |:------: |:------: |:------: |
|CPD<br>MobileNetV2| DUTS+MSRA-B+<br>HKU+NJU2K+MSRA10K| 0.0545 | 0.0485| 0.0389 | 0.0289| 0.0702| 0.1051 |

性能稍微差了一点, 正常

## Results
看一下一些测试集上的效果 (卡了threshold的结果)

![1_rgb](https://raw.githubusercontent.com/YinengXiong/YinengXiong.github.io/master/img/2019-08-15/1_rgb.jpg)
![1_pred](https://raw.githubusercontent.com/YinengXiong/YinengXiong.github.io/master/img/2019-08-15/1_pred.png)

![2_rgb](https://raw.githubusercontent.com/YinengXiong/YinengXiong.github.io/master/img/2019-08-15/2_rgb.jpg)
![2_pred](https://raw.githubusercontent.com/YinengXiong/YinengXiong.github.io/master/img/2019-08-15/2_pred.png)

再看看破烂自然图像(小姐姐)的结果

![3_rgb](https://raw.githubusercontent.com/YinengXiong/YinengXiong.github.io/master/img/2019-08-15/3_rgb.jpg)
![3_pred](https://raw.githubusercontent.com/YinengXiong/YinengXiong.github.io/master/img/2019-08-15/3_pred.png)

![4_rgb](https://raw.githubusercontent.com/YinengXiong/YinengXiong.github.io/master/img/2019-08-15/salient_1.jpg)
![4_pred](https://raw.githubusercontent.com/YinengXiong/YinengXiong.github.io/master/img/2019-08-15/4_pred.png)

感觉效果还不错?

### 暑假学校还要干活？还让不让找工作了(滑稽
好气啊！